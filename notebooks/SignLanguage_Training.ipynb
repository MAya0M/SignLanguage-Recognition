{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sign Language Recognition - Training Notebook\n",
        "\n",
        "This notebook automatically trains the GRU model for sign language recognition.\n",
        "\n",
        "## Quick Start:\n",
        "1. **Runtime ‚Üí Change runtime type ‚Üí Select GPU**\n",
        "2. **Runtime ‚Üí Run all**\n",
        "\n",
        "Or click the \"Open in Colab\" badge in the GitHub repository!\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** Make sure your data is available in the `Data/` directory. If you need to upload data, see `docs/COLAB_UPLOAD_GUIDE.md`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository - delete existing and clone fresh\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Go to /content first\n",
        "os.chdir('/content')\n",
        "print(f\"Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Delete existing SignLanguage-Recognition directory if it exists\n",
        "if os.path.exists('SignLanguage-Recognition'):\n",
        "    print(\"üóëÔ∏è  Deleting existing SignLanguage-Recognition directory...\")\n",
        "    shutil.rmtree('SignLanguage-Recognition')\n",
        "    print(\"‚úÖ Deleted existing directory\")\n",
        "\n",
        "# Clone fresh\n",
        "print(\"üì• Cloning repository...\")\n",
        "os.system('git clone https://github.com/MAya0M/SignLanguage-Recognition.git')\n",
        "print(\"‚úÖ Repository cloned\")\n",
        "\n",
        "# Change to the directory\n",
        "os.chdir('/content/SignLanguage-Recognition')\n",
        "print(f\"‚úÖ Now in: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q tensorflow numpy pandas scikit-learn opencv-python mediapipe tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 0: Re-extract keypoints with MINIMAL normalization (only translate)\n",
        "# This preserves size/rotation differences which help distinguish classes\n",
        "# ‚ö†Ô∏è  This will OVERWRITE all existing keypoint files!\n",
        "print(\"‚ö†Ô∏è  WARNING: This will re-extract ALL keypoints with minimal normalization\")\n",
        "print(\"   This may take a while...\")\n",
        "!python scripts/re_extract_with_minimal_normalization.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç CRITICAL: Check Why Model Stuck at 12.5% (Random Chance)\n",
        "\n",
        "**Run this BEFORE training!** This will diagnose why the model is not learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Check if data from different classes is actually different\n",
        "!python scripts/fix_normalization.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Deep debug - check everything\n",
        "!python scripts/debug_model_training.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q tensorflow numpy pandas scikit-learn opencv-python mediapipe tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Data\n",
        "\n",
        "Make sure your data is in the `Data/` directory. If not, upload it using one of the methods in `docs/COLAB_UPLOAD_GUIDE.md`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data exists and check dataset size\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path('Data')\n",
        "if data_dir.exists():\n",
        "    print(\"‚úÖ Data directory found\")\n",
        "    csv_path = data_dir / 'Labels' / 'dataset.csv'\n",
        "    keypoints_dir = data_dir / 'Keypoints' / 'rawVideos'\n",
        "    \n",
        "    if csv_path.exists():\n",
        "        print(f\"‚úÖ CSV file: {csv_path}\")\n",
        "        # Check dataset size\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"\\nüìä Dataset Statistics:\")\n",
        "        print(f\"   Total samples: {len(df)}\")\n",
        "        print(f\"   Expected: 226 samples (with new videos)\")\n",
        "        \n",
        "        if len(df) < 200:\n",
        "            print(f\"\\n‚ö†Ô∏è  WARNING: Only {len(df)} samples found!\")\n",
        "            print(f\"   Expected 226 samples. Make sure CSV is updated with new videos.\")\n",
        "            print(f\"   Run: !python scripts/create_dataset_csv.py\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ CSV has correct number of samples!\")\n",
        "        \n",
        "        print(f\"\\n   Per label:\")\n",
        "        for label, count in df.groupby('label').size().sort_values(ascending=False).items():\n",
        "            status = \"‚úÖ\" if count >= 25 else \"‚ö†Ô∏è\"\n",
        "            print(f\"      {status} {label:12s}: {count:3d} samples\")\n",
        "        \n",
        "        print(f\"\\n   Per split:\")\n",
        "        for split, count in df.groupby('split').size().items():\n",
        "            print(f\"      {split:6s}: {count:3d} samples\")\n",
        "    else:\n",
        "        print(f\"‚ùå CSV file not found: {csv_path}\")\n",
        "    \n",
        "    if keypoints_dir.exists():\n",
        "        print(f\"\\n‚úÖ Keypoints directory: {keypoints_dir}\")\n",
        "        # Count keypoint files\n",
        "        npy_files = list(keypoints_dir.rglob(\"*.npy\"))\n",
        "        print(f\"   Found {len(npy_files)} keypoint files\")\n",
        "        if len(npy_files) < 200:\n",
        "            print(f\"   ‚ö†Ô∏è  Expected ~226 keypoint files\")\n",
        "    else:\n",
        "        print(f\"‚ùå Keypoints directory not found: {keypoints_dir}\")\n",
        "else:\n",
        "    print(\"‚ùå Data directory not found\")\n",
        "    print(\"Please upload data first! See docs/COLAB_UPLOAD_GUIDE.md\")\n",
        "\n",
        "# Run detailed dataset check\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Running detailed dataset verification...\")\n",
        "print(\"=\"*60)\n",
        "!python scripts/check_dataset.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model with optimized parameters for better learning\n",
        "# FIXED settings for small dataset:\n",
        "# - Higher learning rate (0.002) - faster learning for small dataset\n",
        "# - Smaller batch size (8) - better gradient updates with small dataset\n",
        "# - Smaller model (128 units, 2 layers) - better for small dataset, prevents overfitting\n",
        "# - Less dropout (0.2) - allows more learning with small dataset\n",
        "# - More epochs (200) - give model time to learn\n",
        "# - Normalization DISABLED - keypoints already normalized in extraction\n",
        "\n",
        "!python scripts/train_model.py --csv Data/Labels/dataset.csv --keypoints-dir Data/Keypoints/rawVideos --output-dir models --batch-size 8 --epochs 200 --gru-units 128 --num-gru-layers 2 --dropout 0.2 --learning-rate 0.002 --patience 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model with optimized parameters for better learning\n",
        "# Improved settings:\n",
        "# - Lower learning rate (0.0005) - slightly higher for faster learning\n",
        "# - Smaller batch size (16) for better gradient updates\n",
        "# - More GRU units (256) and layers (3) for better capacity\n",
        "# - Moderate dropout (0.3) - reduced to allow more learning\n",
        "# - More patience (50) - will be tripled internally (150 epochs) to allow model to learn much longer\n",
        "# - More epochs (200) to give model more time to converge\n",
        "# - Early stopping now monitors val_accuracy with very small min_delta to catch any improvement\n",
        "# - Removed batch normalization from dense layers (can interfere with small datasets)\n",
        "\n",
        "!python scripts/train_model.py --csv Data/Labels/dataset.csv --keypoints-dir Data/Keypoints/rawVideos --output-dir models --batch-size 16 --epochs 200 --gru-units 256 --num-gru-layers 3 --dropout 0.3 --learning-rate 0.0005 --patience 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model Quality\n",
        "\n",
        "After training, check the prediction quality:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the latest trained model\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "models = sorted(glob.glob('models/run_*/best_model.keras'))\n",
        "if models:\n",
        "    latest_model = models[-1]\n",
        "    print(f\"üìä Evaluating: {Path(latest_model).name}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Run evaluation\n",
        "    !python scripts/evaluate_model.py --model {latest_model} --csv Data/Labels/dataset.csv --keypoints-dir Data/Keypoints/rawVideos\n",
        "else:\n",
        "    print(\"‚ùå No models found - train the model first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Model (Optional)\n",
        "\n",
        "To save your trained model to Google Drive:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model to your computer\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "models_dir = sorted(glob.glob('models/run_*'))\n",
        "if models_dir:\n",
        "    latest_run = models_dir[-1]  # Latest run\n",
        "    print(f\"üì¶ Preparing model: {Path(latest_run).name}\")\n",
        "    \n",
        "    # Create a zip file\n",
        "    zip_name = f\"{Path(latest_run).name}\"\n",
        "    shutil.make_archive(zip_name, 'zip', latest_run)\n",
        "    \n",
        "    # Download\n",
        "    print(f\"‚¨áÔ∏è Downloading {zip_name}.zip...\")\n",
        "    files.download(f'{zip_name}.zip')\n",
        "    print(\"‚úÖ Model downloaded! Extract and add to your GitHub repo.\")\n",
        "else:\n",
        "    print(\"‚ùå No models found - train the model first!\")\n",
        "\n",
        "# Alternative: Save to Google Drive (uncomment to use)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# dest = f'/content/drive/MyDrive/{Path(latest_run).name}'\n",
        "# shutil.copytree(latest_run, dest, dirs_exist_ok=True)\n",
        "# print(f\"‚úÖ Model saved to Google Drive: {Path(latest_run).name}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
