{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sign Language Recognition - Training Notebook\n",
        "\n",
        "This notebook automatically trains the **CNN + LSTM** model for sign language recognition.\n",
        "\n",
        "## Quick Start:\n",
        "1. **Runtime → Change runtime type → Select GPU**\n",
        "2. **Runtime → Run all**\n",
        "\n",
        "Or click the \"Open in Colab\" badge in the GitHub repository!\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** Make sure your data is available in the `Data/` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository - delete existing and clone fresh\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Go to /content first\n",
        "os.chdir('/content')\n",
        "print(f\"Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Delete existing SignLanguage-Recognition directory if it exists\n",
        "if os.path.exists('SignLanguage-Recognition'):\n",
        "    print(\"  Deleting existing SignLanguage-Recognition directory...\")\n",
        "    shutil.rmtree('SignLanguage-Recognition')\n",
        "    print(\" Deleted existing directory\")\n",
        "\n",
        "# Clone fresh\n",
        "print(\" Cloning repository...\")\n",
        "os.system('git clone https://github.com/MAya0M/SignLanguage-Recognition.git')\n",
        "print(\" Repository cloned\")\n",
        "\n",
        "# Change to the directory\n",
        "os.chdir('/content/SignLanguage-Recognition')\n",
        "print(f\"Now in: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q tensorflow numpy pandas scikit-learn opencv-python mediapipe tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Re-extract keypoints with MINIMAL normalization (only translate)\n",
        "# This preserves size/rotation differences which help distinguish classes\n",
        "#   This will OVERWRITE all existing keypoint files!\n",
        "# Uncomment the line below if you want to re-extract keypoints:\n",
        "# !python scripts/re_extract_with_minimal_normalization.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load Data from tar.gz (Google Drive)\n",
        "\n",
        "# Load sign_language_data.tar.gz from Google Drive\n",
        "# Make sure you uploaded sign_language_data.tar.gz to /content/drive/MyDrive/\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy tar.gz from Drive to current directory\n",
        "!cp /content/drive/MyDrive/sign_language_data.tar.gz .\n",
        "\n",
        "# Extract the tar.gz file\n",
        "!tar -xzf sign_language_data.tar.gz\n",
        "\n",
        "# Check if Data directory exists\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "if os.path.exists('Data'):\n",
        "    print(\"Data directory found\")\n",
        "    print(f\"   Path: {os.path.abspath('Data')}\")\n",
        "    \n",
        "    # Check keypoints count\n",
        "    keypoints_dir = Path('Data/Keypoints/rawVideos')\n",
        "    if keypoints_dir.exists():\n",
        "        total_keypoints = len(list(keypoints_dir.rglob(\"*.npy\")))\n",
        "        print(f\"   Found {total_keypoints} keypoint files\")\n",
        "        \n",
        "        # Count per label\n",
        "        print(f\"\\n   Keypoints per label:\")\n",
        "        for label_dir in sorted(keypoints_dir.iterdir()):\n",
        "            if label_dir.is_dir():\n",
        "                npy_count = len(list(label_dir.glob(\"*.npy\")))\n",
        "                if npy_count > 0:\n",
        "                    print(f\"      {label_dir.name:12s}: {npy_count:3d} files\")\n",
        "        \n",
        "        if total_keypoints >= 300:\n",
        "            print(f\"\\n    Data loaded successfully! ({total_keypoints} keypoint files)\")\n",
        "        else:\n",
        "            print(f\"\\n     Expected ~306 keypoint files, found {total_keypoints}\")\n",
        "            print(f\"      Make sure you're using the updated sign_language_data.tar.gz\")\n",
        "    else:\n",
        "        print(\"     Keypoints directory not found\")\n",
        "else:\n",
        "    print(\"  Data directory not found!\")\n",
        "    print(\"   Please check that sign_language_data.tar.gz is in Google Drive at /content/drive/MyDrive/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Verify Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data exists and check dataset size + class imbalance\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "data_dir = Path('Data')\n",
        "if data_dir.exists():\n",
        "    print(\" Data directory found\")\n",
        "    csv_path = data_dir / 'Labels' / 'dataset.csv'\n",
        "    keypoints_dir = data_dir / 'Keypoints' / 'rawVideos'\n",
        "    \n",
        "    if csv_path.exists():\n",
        "        print(f\" CSV file: {csv_path}\")\n",
        "        # Check dataset size\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"\\n Dataset Statistics:\")\n",
        "        print(f\"   Total samples: {len(df)}\")\n",
        "        print(f\"   Number of classes: {df['label'].nunique()}\")\n",
        "        \n",
        "        print(f\"\\n   Per label:\")\n",
        "        label_counts = df.groupby('label').size().sort_values(ascending=False)\n",
        "        max_count = label_counts.max()\n",
        "        min_count = label_counts.min()\n",
        "        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "        most_common_label = label_counts.index[0]\n",
        "        \n",
        "        for label, count in label_counts.items():\n",
        "            status = \"good\" if count >= 20 else \"bad\"\n",
        "            if count == max_count:\n",
        "                status = \"badd\"  # Most common (might cause issues)\n",
        "            print(f\"      {status} {label:12s}: {count:3d} samples ({(count/len(df)*100):5.2f}%)\")\n",
        "        \n",
        "        print(f\"\\n     Class Imbalance Analysis:\")\n",
        "        print(f\"      Most common: {most_common_label} ({max_count} samples)\")\n",
        "        print(f\"      Least common: {label_counts.index[-1]} ({min_count} samples)\")\n",
        "        print(f\"      Imbalance ratio: {imbalance_ratio:.2f}x\")\n",
        "        \n",
        "        if imbalance_ratio > 2.0:\n",
        "            print(f\"\\n       WARNING: Significant class imbalance detected!\")\n",
        "            print(f\"         The model might favor '{most_common_label}' (majority class).\")\n",
        "            print(f\"         This is why the model predicts '{most_common_label}' on everything.\")\n",
        "            print(f\"          Solution: Class weights will be used during training.\")\n",
        "            print(f\"          Recommendation: Add more samples to minority classes (aim for 30-50 per class)\")\n",
        "        else:\n",
        "            print(f\"      Classes are relatively balanced\")\n",
        "        \n",
        "        print(f\"\\n   Per split:\")\n",
        "        for split in ['train', 'val', 'test']:\n",
        "            split_df = df[df['split'] == split]\n",
        "            print(f\"      {split:6s}: {len(split_df):3d} samples\")\n",
        "            \n",
        "            # Show per-label distribution in split\n",
        "            split_label_counts = split_df.groupby('label').size().sort_values(ascending=False)\n",
        "            for label, count in split_label_counts.items():\n",
        "                print(f\"         {label:12s}: {count:3d}\")\n",
        "    else:\n",
        "        print(f\" CSV file not found: {csv_path}\")\n",
        "        print(f\"   Run: !python scripts/create_dataset_csv.py\")\n",
        "    \n",
        "    if keypoints_dir.exists():\n",
        "        print(f\"\\n Keypoints directory: {keypoints_dir}\")\n",
        "        # Count keypoint files\n",
        "        npy_files = list(keypoints_dir.rglob(\"*.npy\"))\n",
        "        print(f\"   Found {len(npy_files)} keypoint files\")\n",
        "        \n",
        "        # Count per label\n",
        "        print(f\"\\n   Keypoints per label (actual files on disk):\")\n",
        "        keypoint_counts = {}\n",
        "        for label_dir in sorted(keypoints_dir.iterdir()):\n",
        "            if label_dir.is_dir():\n",
        "                npy_count = len(list(label_dir.glob(\"*.npy\")))\n",
        "                if npy_count > 0:\n",
        "                    keypoint_counts[label_dir.name] = npy_count\n",
        "                    print(f\"      {label_dir.name:12s}: {npy_count:3d} files\")\n",
        "        \n",
        "        # Compare with CSV\n",
        "        if csv_path.exists():\n",
        "            print(f\"\\n   Comparison (CSV vs Keypoints):\")\n",
        "            csv_label_counts = df.groupby('label').size().to_dict()\n",
        "            for label_name in sorted(keypoint_counts.keys()):\n",
        "                keypoint_count = keypoint_counts[label_name]\n",
        "                # Find matching label in CSV (case-insensitive)\n",
        "                csv_label = None\n",
        "                csv_count = 0\n",
        "                for csv_lbl in csv_label_counts.keys():\n",
        "                    if csv_lbl.upper() == label_name.upper():\n",
        "                        csv_label = csv_lbl\n",
        "                        csv_count = csv_label_counts[csv_lbl]\n",
        "                        break\n",
        "                \n",
        "                if csv_count > 0:\n",
        "                    status = \"OK\" if keypoint_count == csv_count else \"WARNING\"\n",
        "                    print(f\"      {status} {label_name:12s}: CSV={csv_count:3d}, Keypoints={keypoint_count:3d}\")\n",
        "                    if keypoint_count != csv_count:\n",
        "                        print(f\"         WARNING: Mismatch! CSV has {csv_count} but {keypoint_count} keypoint files exist\")\n",
        "                else:\n",
        "                    print(f\"      WARNING: {label_name:12s}: {keypoint_count:3d} keypoint files, but not in CSV!\")\n",
        "    else:\n",
        "        print(f\"Keypoints directory not found: {keypoints_dir}\")\n",
        "        print(f\"   Run: !python scripts/extract_keypoints.py\")\n",
        "else:\n",
        "    print(\"Data directory not found\")\n",
        "    print(\"Please upload data first!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CNN + LSTM model\n",
        "# Architecture: CNN (spatial patterns) + LSTM (temporal patterns)\n",
        "# \n",
        "# NEW: Class weights are automatically used to handle class imbalance\n",
        "# NEW: Smart frame sampling (skips similar start, focuses on gesture)\n",
        "#\n",
        "# Optimized parameters:\n",
        "# - Batch size: 8 (better for small dataset)\n",
        "# - Epochs: 200 (give model time to learn)\n",
        "# - CNN filters: 64 (first layer), 128 (second layer)\n",
        "# - LSTM units: 128\n",
        "# - Dropout: 0.3 (prevent overfitting)\n",
        "# - Learning rate: 0.001\n",
        "# - Class weights: Automatic (balanced) - handles class imbalance\n",
        "\n",
        "!python scripts/train_model.py \\\n",
        "    --csv Data/Labels/dataset.csv \\\n",
        "    --keypoints-dir Data/Keypoints/rawVideos \\\n",
        "    --output-dir models \\\n",
        "    --batch-size 8 \\\n",
        "    --epochs 200 \\\n",
        "    --cnn-filters 64 \\\n",
        "    --lstm-units 128 \\\n",
        "    --num-cnn-layers 2 \\\n",
        "    --dropout 0.3 \\\n",
        "    --learning-rate 0.001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Training Results\n",
        "\n",
        "After training, check the results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check training results\n",
        "import glob\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "models_dir = sorted(glob.glob('models/run_*'))\n",
        "if models_dir:\n",
        "    latest_run = models_dir[-1]\n",
        "    print(f\" Latest training run: {Path(latest_run).name}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Check test results\n",
        "    test_results_path = Path(latest_run) / \"test_results.json\"\n",
        "    if test_results_path.exists():\n",
        "        with open(test_results_path, 'r') as f:\n",
        "            results = json.load(f)\n",
        "        print(f\"\\n Test Results:\")\n",
        "        print(f\"   Accuracy: {results['test_accuracy']:.2%}\")\n",
        "        print(f\"   Loss: {results['test_loss']:.4f}\")\n",
        "    else:\n",
        "        print(\"WARNING: Test results not found - model may still be training\")\n",
        "    \n",
        "    # Check training history\n",
        "    history_path = Path(latest_run) / \"training_history.json\"\n",
        "    if history_path.exists():\n",
        "        with open(history_path, 'r') as f:\n",
        "            history = json.load(f)\n",
        "        if 'val_accuracy' in history:\n",
        "            best_val_acc = max(history['val_accuracy'])\n",
        "            print(f\"\\n Best Validation Accuracy: {best_val_acc:.2%}\")\n",
        "else:\n",
        "    print(\" No models found - train the model first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Model (Optional)\n",
        "\n",
        "To save your trained model to Google Drive:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model to your computer\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "models_dir = sorted(glob.glob('models/run_*'))\n",
        "if models_dir:\n",
        "    latest_run = models_dir[-1]  # Latest run\n",
        "    print(f\" Preparing model: {Path(latest_run).name}\")\n",
        "    \n",
        "    # Create a zip file\n",
        "    zip_name = f\"{Path(latest_run).name}\"\n",
        "    shutil.make_archive(zip_name, 'zip', latest_run)\n",
        "    \n",
        "    # Download\n",
        "    print(f\"Downloading {zip_name}.zip...\")\n",
        "    files.download(f'{zip_name}.zip')\n",
        "    print(\"Model downloaded! Extract and add to your GitHub repo.\")\n",
        "else:\n",
        "    print(\"No models found - train the model first!\")\n",
        "\n",
        "# Alternative: Save to Google Drive (uncomment to use)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# dest = f'/content/drive/MyDrive/{Path(latest_run).name}'\n",
        "# shutil.copytree(latest_run, dest, dirs_exist_ok=True)\n",
        "# print(f\"Model saved to Google Drive: {Path(latest_run).name}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
